---
title: "Class 17"
author: "Tobias Gerstenberg"
date: "February 20th, 2019"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
    pandoc_args: ["--number-offset=16"]
bibliography: [packages.bib]
nocite: '@*'
---

```{r setup, include=FALSE}
# these options here change the formatting of how comments are rendered
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  fig.show = "hold")
```

# Model comparison

```{r install-packages, include=FALSE, eval=FALSE}
install.packages(c("lme4", "lmerTest", "pbkrtest"))
```

## Load packages and set plotting theme  

```{r load-packages, message=FALSE}
library("knitr")      # for knitting RMarkdown 
library("kableExtra") # for making nice tables
library("janitor")    # for cleaning column names
library("broom")    # for tidying up linear models 
library("patchwork")    # for making figure panels
library("lme4")    # for linear mixed effects models
# library("lmerTest")    # provides p-values for lmers
library("pbkrtest")    # for testing lmers, parametric bootstrap and modified F-test
library("tidyverse")  # for wrangling, plotting, etc. 

# include references for used packages
knitr::write_bib(.packages(), "packages.bib") 
```

```{r set-theme}
theme_set(
  theme_classic() + #set the theme 
    theme(text = element_text(size = 20)) #set the default text size
)
```

# Linear mixed effects models 1 

## Things that came up in class 

### Comparing t-test with F-test in `lm()`

```{r}
# make example reproducible 
set.seed(1)

# parameters
sample_size = 100
b0 = 1
b1 = 0.5
b2 = 0.5
sd = 0.5

# sample
df.data = tibble(
  participant = 1:sample_size,
  x1 = runif(sample_size, min = 0, max = 1),
  x2 = runif(sample_size, min = 0, max = 1),
  # simple additive model
  y = b0 + b1 * x1 + b2 * x2 + rnorm(sample_size, sd = sd) 
) 

# fit linear model 
fit = lm(formula = y ~ 1 + x1 + x2,
   data = df.data)

# print model summary 
fit %>% summary()
```

Let's visualize the data: 

```{r}
df.data %>% 
  ggplot(data = .,
         mapping = aes(x = x1,
                       y = y,
                       color = x2)) +
  geom_smooth(method = "lm",
              color = "black") + 
  geom_point()
  
```


The global F-test which is shown by the F-statistic at the bottom of the `summary()` output compares the full model with a  model that only has an intercept. So, to use our model comparison approach, we would compare the following two models 

```{r}
# fit models 
model_compact = lm(formula = y ~ 1,
                   data = df.data)

model_augmented = lm(formula = y ~ 1 + x1 + x2,
                     data = df.data)

# compare models using the F-test
anova(model_compact, model_augmented)

```

Note how the result of the F-test using the `anova()` function which compares the two models is identical to the F-statistic reported at the end of the `summary` function.

To test for individual parameters in the model, we compare two models, a compact model without that parameter, and an augmented model with that parameter. Let's test the significance of `x1`. 

```{r}
# fit models 
model_compact = lm(formula = y ~ 1 + x2,
                   data = df.data)

model_augmented = lm(formula = y ~ 1 + x1 + x2,
                     data = df.data)

# compare models using the F-test
anova(model_compact, model_augmented)
```

Note how the p-value that we get from the F-test is equivalent to the one that we get from the t-test reported in the `summary()` function. The F-test statistic (in the `anova()` result) and the t-value (in the `summary()` of the linear model) are deterministically related. In fact, the relationship is just: 

$$
t = \sqrt{F}
$$

Let's check that that's correct: 

```{r, warning=FALSE}
# get the t-value from the fitted lm
t_value = fit %>% 
  tidy() %>% 
  filter(term == "x1") %>% 
  pull(statistic)

# get the F-value from comparing the compact model (without x1) with the 
# augmented model (with x1)

f_value = anova(model_compact, model_augmented) %>% 
  tidy() %>% 
  pull(statistic) %>% 
  .[2]

# t-value 
print(str_c("t_value: ", t_value))

# square root of f_value 
print(str_c("sqrt of f_value: ", sqrt(f_value)))
```

Yip, they are the same. 

## Dependence 

```{r}
# make example reproducible 
set.seed(1)

df.dependence = data_frame(
  participant = 1:20,
  condition1 = rnorm(20),
  condition2 = condition1 + rnorm(20, mean = 0.2, sd = 0.1)
) %>% 
  mutate(condition2shuffled = sample(condition2))

df.plot = df.dependence %>% 
  gather("condition", "value", -participant) %>% 
  mutate(condition = str_replace(condition, "condition", ""))

p1 = ggplot(data = df.plot %>% filter(condition != "2shuffled"), 
            mapping = aes(x = condition, y = value)) +
  geom_line(aes(group = participant), alpha = 0.3) +
  geom_point() +
  stat_summary(fun.y = "mean", 
               geom = "point",
               shape = 21, 
               fill = "red",
               size = 4) +
  labs(title = "original",
       tag = "a)")
  
p2 = ggplot(data = df.plot %>% filter(condition != "2"), 
            mapping = aes(x = condition, y = value)) +
  geom_line(aes(group = participant), alpha = 0.3) +
  geom_point() +
  stat_summary(fun.y = "mean", 
               geom = "point",
               shape = 21, 
               fill = "red",
               size = 4) +
  labs(title = "shuffled",
       tag = "b)")

p1 + p2 

```

```{r}
# separate the data sets 
df.original = df.dependence %>% 
  gather("condition", "value", -participant) %>% 
  mutate(condition = str_replace(condition, "condition", "")) %>% 
  filter(condition != "2shuffled")
  
df.shuffled = df.dependence %>% 
  gather("condition", "value", -participant) %>% 
  mutate(condition = str_replace(condition, "condition", "")) %>% 
  filter(condition != "2")
```

```{r}
# linear model (assuming independent samples)
lm(formula = value ~ condition,
   data = df.original) %>% 
  summary() 

t.test(df.original$value[df.original$condition == "1"],
       df.original$value[df.original$condition == "2"],
       alternative = "two.sided",
       paired = F
       )
```

The mean difference between the conditions is very small 

```{r}
# fit a linear mixed effects model 
lmer(formula = value ~ condition + (1 | participant),
                 data = df.original) %>% 
  summary()
```


```{r}
# fit models
fit.compact = lmer(formula = value ~ 1 + (1 | participant),
                 data = df.original)
fit.augmented = lmer(formula = value ~ condition + (1 | participant),
                 data = df.original)

# compare via Chisq-test
anova(fit.compact, fit.augmented)

t.test(df.original$value[df.original$condition == "1"],
       df.original$value[df.original$condition == "2"],
       alternative = "two.sided",
       paired = T
       )
```

Visualize model predictions: 

```{r}
# model assuming independence
fit.independent = lm(formula = value ~ 1 + condition,
                     data = df.original)

# model assuming dependence
fit.dependent = lmer(formula = value ~ 1 + condition + (1 | participant),
                     data = df.original)
```

Model predictions:

```{r}
# plot with predictions by fit.independent 
fit.independent %>% 
  augment() %>% 
  bind_cols(df.original %>% select(participant)) %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") + 
  geom_line(aes(y = fitted),
             color = "red")
```

Illustrate residuals: 

```{r}
# make example reproducible 
set.seed(1)

fit.independent %>% 
  augment() %>% 
  bind_cols(df.original %>% select(participant)) %>% 
  clean_names() %>% 
  mutate(index = as.numeric(condition),
         index = index + runif(n(), min = -0.3, max = 0.3)) %>% 
  ggplot(data = .,
         mapping = aes(x = index,
                       y = value,
                       group = participant,
                       color = condition)) +
  geom_point() + 
  geom_smooth(method = "lm",
              se = F,
              formula = "y ~ 1",
              aes(group = condition)) +
  geom_segment(aes(xend = index,
                   yend = fitted),
               alpha = 0.5) +
  scale_color_brewer(palette = "Set1") +
  scale_x_continuous(breaks = 1:2, 
                     labels = 1:2) +
  labs(x = "condition") +
  theme(legend.position = "none")

```


```{r}
# plot with predictions by fit.independent 
fit.dependent %>% 
  augment() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") + 
  geom_line(aes(y = fitted),
             color = "red")
```

Residual plots

```{r}
fit.independent %>% 
  augment() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = fitted,
                       y = resid)) +
  geom_point() +
  coord_cartesian(ylim = c(-2.5, 2.5))

```

```{r}
fit.dependent %>% 
  augment() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = fitted,
                       y = resid)) +
  geom_point() + 
  coord_cartesian(ylim = c(-2.5, 2.5))
```

Compare whether accounting for dependence is worth it: 

```{r}
# fit models (without and with dependence)
fit.compact = lm(formula = value ~ 1 + condition,
                 data = df.original)

fit.augmented = lmer(formula = value ~ 1 + condition + (1 | participant),
                     data = df.original)

# compare models
# note: the lmer model has to be supplied first 
anova(fit.augmented, fit.compact) 
```


Explore lmer() object: 

```{r}
fit.augmented %>% 
  summary()
```


```{r}
fit.augmented %>% 
  tidy()
```

## Explore stuff

### No correlation between fixed effects when predictor is effect coded

```{r}
df.test = df.original %>% 
  mutate(condition = as.factor(condition))
  
contrasts(df.test$condition) = "contr.sum"

fit.test = lmer(formula = value ~ 1 + condition + (1 | participant),
                     data = df.test)

fit.test %>% 
  augment() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") + 
  geom_line(aes(y = fitted),
             color = "red")

fit.test %>% summary()
```

### no correlation when effect of condition doesn't vary 

- no correlation but also fails to converge 

```{r}
# make example reproducible 
set.seed(1)

tmp = rnorm(n = 20)

df.test = tibble(
  condition = rep(1:2, each = 20), 
  participant = rep(1:20, 2),
  value = ifelse(condition == 1, tmp,
                 tmp + 3)
)

lmer(formula = value ~ condition + (1 | participant),
     data = df.test) %>% 
  summary()
```

### regression to the mean example 

- lmer() with random intercepts not better than lm() even though there is individual variation
- 

```{r}
# make example reproducible 
set.seed(1)

tmp = rnorm(n = 20)

df.test = tibble(
  condition = rep(1:2, each = 20), 
  participant = rep(1:20, 2),
  value = ifelse(condition == 1, tmp,
                 mean(tmp) + rnorm(n = 20, sd = 0.3))
                 # tmp + rnorm(n = 20, sd = 0.3))
) %>% 
  mutate(condition = as.factor(condition),
         participant = as.factor(participant))

fit.test = lmer(formula = value ~ 1 + condition + (1 | participant),
                data = df.test)

fit.test %>% 
  augment() %>% 
  clean_names() %>% 
  bind_cols(df.test %>% select(participant)) %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") + 
  geom_line(aes(y = fitted),
             color = "red")

fit.test %>% summary()
```

### fixed effects factor not worth it 

- little variance between individual participants 

```{r}
# make example reproducible 
set.seed(1)

# parameters 
sample_size = 100
b0 = 1
b1 = 2
sd_residual = 1
sd_participant = 0.5

# randomly draw intercepts for each participant
intercepts = rnorm(sample_size, sd = sd_participant)

# generate the data 
df.test = tibble(
  condition = rep(0:1, each = sample_size), 
  participant = rep(1:sample_size, 2)) %>% 
  group_by(condition) %>% 
  mutate(value = b0 + b1 * condition + intercepts + rnorm(n(), sd = sd_residual)) %>% 
  ungroup %>% 
  mutate(condition = as.factor(condition),
         participant = as.factor(participant))

fit.test = lmer(formula = value ~ 1 + condition + (1 | participant),
                data = df.test)

fit.test %>% 
  augment() %>% 
  clean_names() %>% 
  bind_cols(df.test %>% select(participant)) %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") +
  geom_line(aes(y = fitted),
             color = "red")

fit.test %>% summary()

# fit linear model 
fit.lm = lm(formula = value ~ 1 + condition,
            data = df.test)

# chisq test 
anova(fit.test, fit.lm) 

fit.test %>% 
  glance()

fit.test %>% 
  tidy()

fit.lm %>% 
  glance()
```



### Compare lmerTest with chisq test: 

```{r}
# using lmerTest
fit.lmer = lmerTest::lmer(formula = value ~ condition + (1 | participant),
                data = df.original)

fit.lmer %>% 
  summary()

# using chisq test
fit.compact = lm(formula = value ~ 1 +  condition,
                data = df.original)

fit.augmented = lmer(formula = value ~ 1 + condition +  (1 | participant),
                data = df.original)

anova(fit.augmented, fit.compact)

# similar method 
lmerTest::rand(fit.lmer)

```

### coefficients 

```{r}

```


### outlier

- sensitivity to outliers 
- strongly affects predictions (simulated data)

```{r}
# make example reproducible 
set.seed(1)

sample_size = 20
b0 = 1
b1 = 2
sd_residual = 0.5
sd_participant = 0.5

# randomly draw intercepts for each participant
intercepts = rnorm(sample_size, sd = sd_participant)

df.test = tibble(
  condition = rep(0:1, each = sample_size), 
  participant = rep(1:sample_size, 2)) %>% 
  group_by(condition) %>% 
  mutate(value = b0 + b1 * condition + intercepts + rnorm(n(), sd = sd_residual)) %>% 
  ungroup %>% 
  mutate(condition = as.factor(condition),
         participant = as.factor(participant))

df.test = df.test %>%
  mutate(value = ifelse(participant == 20, value + 30, value))

# fit model
fit.test = lmer(formula = value ~ 1 + condition + (1 | participant),
                data = df.test)

fit.test %>%
  augment() %>%
  clean_names() %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") +
  geom_line(aes(y = fitted),
             color = "red")

fit.test %>% summary()

# simulated data 
fit.test %>% 
  simulate() %>% 
  bind_cols(df.test) %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = sim_1,
                       group = participant)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.5)

```


### predict and simulate from lmer 

```{r}
fit.lmer = lmer(formula = value ~ condition + (1 | participant),
                data = df.original)

# # deterministic 
fit.lmer %>%
  predict(newdata = tibble(condition = as.factor(rep(c(1:2), each = 13)),
                           participant = rep(10:22, 2)),
          allow.new.levels = T)

# condition on the fit random effects (or not)
fit.lmer %>%
  simulate(newdata = tibble(condition = as.factor(rep(c(1:2), each = 13)),
                           participant = rep(10:22, 2)),
          allow.new.levels = T,
          re.form = NULL)

fit.lmer %>% 
  simulate(nsim = 100) %>% 
  bind_cols(df.original) %>% 
  # filter(participant == 1) %>% 
  filter(participant == 14) %>% 
  gather("simulation", "prediction", -c(participant, condition, value)) %>% 
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = prediction,
                       group = simulation)) +
  geom_line(alpha = 0.5) + 
  geom_line(aes(y = value), 
            color = "red",
            size = 2)

tmp = fit.lmer %>% 
  simulate(nsim = 100)
  
```
### paired t-test 

```{r}
# original data set 
t.test(df.original$value[df.original$condition == "1"],
       df.original$value[df.original$condition == "2"],
       alternative = "two.sided",
       paired = T)

# equivalent: t-test on the differences 
df.original %>% 
  spread(condition, value) %>% 
  mutate(diff = `1` - `2`) %>% 
  pull(diff) %>% 
  t.test()
  
# shuffled data set 
t.test(df.shuffled$value[df.shuffled$condition == "1"],
       df.shuffled$value[df.shuffled$condition == "2"],
       alternative = "two.sided",
       paired = T
       )
```

### model coefficients 

```{r}
fit.independent %>% coef()

fit.dependent %>% coef()
fit.dependent %>% ranef()
```


### lm on differences 

```{r}
df.original %>% 
  spread(condition, value) %>% 
  mutate(diff = `1` - `2`) %>% 
  lm(formula = diff ~ 1,
     data = .) %>% 
  summary()
```

- can't generate new data from this approach (since the interindividual variation is not modeled)



## Additional resources 

### Readings 

- [Linear mixed effects models tutorial by Bodo Winter](https://arxiv.org/pdf/1308.5499.pdf)

## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r session}
sessionInfo()
```

## References