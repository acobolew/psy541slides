---
title: "Class 23"
author: "Tobias Gerstenberg"
date: "03/08/2018"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
    pandoc_args: ["--number-offset=22"]
bibliography: [packages.bib]
nocite: '@*'
---

```{r setup, include=FALSE}
# these options here change the formatting of how comments are rendered
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  fig.show = "hold")
```

# Bayesian data analysis 3

## Load packages and set plotting theme  

```{r load-packages, message=FALSE}
library("knitr")      # for knitting RMarkdown 
library("kableExtra") # for making nice tables
library("janitor")    # for cleaning column names
library("modelr")    # for doing modeling stuff
library("tidybayes")    # tidying up results from Bayesian models
library("brms")    # Bayesian regression models with Stan
library("rstanarm")    # for Bayesian models
library("cowplot")    # for making figure panels
library("ggrepel") # for labels in ggplots
library("gganimate") # for animations
# library("extraDistr") # additional probability distributions
library("GGally")  # for pairs plot
library("bayesplot")  # for visualization of Bayesian model fits 
library("tidyverse")  # for wrangling, plotting, etc. 

# include references for used packages
knitr::write_bib(.packages(), "packages.bib") 
```

```{r set-theme}
theme_set(
  theme_classic() + #set the theme 
    theme(text = element_text(size = 20)) #set the default text size
)
```

## Load data set 

```{r}
df.poker = read_csv("data/poker.csv") %>% 
  mutate(skill = factor(skill,
                        levels = 1:2,
                        labels = c("expert", "average")),
         skill = fct_relevel(skill, "average", "expert"),
         hand = factor(hand,
                       levels = 1:3,
                       labels = c("bad", "neutral", "good")),
         limit = factor(limit,
                        levels = 1:2,
                        labels = c("fixed", "none")),
         participant = 1:n()) %>% 
  select(participant, everything())
```


## Poker 

### Data visualization

```{r}
df.poker %>% 
  ggplot(mapping = aes(x = hand,
                       y = balance,
                       fill = hand)) + 
  geom_point(alpha = 0.2,
             position = position_jitter(height = 0, width = 0.1)) + 
  stat_summary(fun.data = "mean_cl_boot",
               geom = "linerange",
               size = 1) + 
  stat_summary(fun.y = "mean",
               geom = "point",
               shape = 21,
               size = 4) +
  labs(y = "final balance (in Euros)") + 
  scale_fill_manual(values = c("red", "orange", "green")) +
  theme(legend.position = "none")
```

### Linear model 

```{r}
fit.lm = lm(formula = balance ~ 1 + hand,
            data = df.poker)

fit.lm %>% summary()
```

### Bayesian model 

```{r}
fit.brm = brm(formula = balance ~ 1 + hand,
              data = df.poker)

fit.brm %>% summary()
```

#### Visualize the posteriors 

```{r}
fit.brm %>% get_variables()
```


```{r}
# fit.brm %>%
fit.brm %>% 
  posterior_samples() %>% 
  select(-lp__) %>% 
  gather("variable", "value") %>% 
  ggplot(data = .,
         mapping = aes(y = variable, x = value)) +
  geom_halfeyeh()
```

```{r}
fit.brm %>% 
  posterior_samples() %>% 
  select(b_Intercept:sigma) %>% 
  ggpairs(lower = list(continuous = wrap("points", alpha = 0.03)),
          upper = list(continuous = wrap("cor", size = 6))) + 
  theme(panel.grid.major = element_blank(),
        text = element_text(size = 12))
```

#### Compute highest density intervals 

```{r}
fit.brm %>% 
  posterior_samples() %>% 
  clean_names() %>% 
  select(starts_with("b_"), sigma) %>% 
  mode_hdi() %>% 
  gather("index", "value", -c(.width:.interval)) %>% 
  select(index, value) %>% 
  mutate(index = ifelse(str_detect(index, fixed(".")), index, str_c(index, ".mode"))) %>% 
  separate(index, into = c("parameter", "type"), sep = "\\.") %>% 
  spread(type, value) %>% 
    head(10) %>% 
    kable(digits = 2) %>% 
    kable_styling(bootstrap_options = "striped",
                full_width = F)
  
```


#### Test hypothesis

```{r}
# fit.brm %>%
fit.brm %>% 
  posterior_samples() %>% 
  select(b_handneutral) %>% 
  gather("variable", "value") %>% 
  ggplot(data = .,
         mapping = aes(y = variable, x = value)) +
  geom_halfeyeh() + 
  geom_vline(xintercept = 4,
             color = "red")
```

```{r}
hypothesis(fit.brm,
           hypothesis = "handneutral < 0")

hypothesis(fit.brm,
           hypothesis = "handneutral > 4")

hypothesis(fit.brm,
           hypothesis = "Intercept +  > 2 * Intercept")

hypothesis(fit.brm,
           hypothesis = "Intercept + handneutral < (Intercept + Intercept + handgood) / 2")
```

Let's double check one example

```{r}
df.hypothesis = fit.brm %>% 
  posterior_samples() %>% 
  clean_names() %>% 
  select(starts_with("b_")) %>% 
  mutate(neutral = b_intercept + b_handneutral,
         bad_good_average = (b_intercept + b_intercept + b_handgood)/2,
         hypothesis = neutral < bad_good_average)

df.hypothesis %>% 
  summarize(p = sum(hypothesis)/n())
         
```

#### Bayes factor 

```{r}
fit.brm1 = brm(formula = balance ~ 1 + hand,
               data = df.poker,
               save_all_pars = T,
               file = "brm_factor1")

fit.brm2 = brm(formula = balance ~ 1 + hand + skill,
               data = df.poker,
               save_all_pars = T,
               file = "brm_factor2")
```

```{r}
bayes_factor(fit.brm2, fit.brm1)
```


#### Posterior predictive check 

```{r}
pp_check(fit.brm, nsamples = 100)
```

```{r}
df.predictive_samples = fit.brm %>% 
  posterior_samples() %>% 
  clean_names() %>% 
  select(contains("b_"), sigma) %>% 
  sample_n(size = 20) %>% 
  mutate(sample = 1:n()) %>% 
  group_by(sample) %>% 
  nest() %>% 
  mutate(bad = map(data, ~ .$b_intercept + rnorm(100, sd = .$sigma)),
         neutral = map(data, ~ .$b_intercept + .$b_handneutral + rnorm(100, sd = .$sigma)),
         good = map(data, ~ .$b_intercept + .$b_handgood + rnorm(100, sd = .$sigma))) %>% 
  unnest(bad, neutral, good)
```

Visualization: 

```{r}

p = df.predictive_samples %>% 
  gather("hand", "balance", -sample) %>% 
  mutate(hand = factor(hand, levels = c("bad", "neutral", "good"))) %>% 
  ggplot(mapping = aes(x = hand,
                       y = balance,
                       fill = hand)) + 
  geom_point(alpha = 0.2,
             position = position_jitter(height = 0, width = 0.1)) + 
  stat_summary(fun.data = "mean_cl_boot",
               geom = "linerange",
               size = 1) + 
  stat_summary(fun.y = "mean",
               geom = "point",
               shape = 21,
               size = 4) +
  labs(y = "final balance (in Euros)") + 
  scale_fill_manual(values = c("red", "orange", "green")) +
  theme(legend.position = "none") + 
  transition_manual(sample)

animate(p, nframes = 120, width = 800, height = 600, res = 96, type = "cairo")

# anim_save("poker_posterior_predictive.gif")
```

#### Full specification

```{r}
fit.brm2 = brm(
  formula = balance ~ 1 + hand,
  family = "gaussian",
  data = df.poker,
  prior = c(
    prior(normal(0, 10), class = "b", coef = "handgood"),
    prior(normal(0, 10), class = "b", coef = "handneutral"),
    prior(student_t(3, 3, 10), class = "Intercept"),
    prior(student_t(3, 0, 10), class = "sigma")
  ),
  inits = list(
    list(Intercept = 0, sigma = 1, handgood = 5, handneutral = 5),
    list(Intercept = -5, sigma = 3, handgood = 2, handneutral = 2),
    list(Intercept = 2, sigma = 1, handgood = -1, handneutral = 1),
    list(Intercept = 1, sigma = 2, handgood = 2, handneutral = -2)
  ),
  iter = 4000,
  warmup = 1000,
  chains = 4,
  file = "brm2",
  seed = 1
)

fit.brm2 %>% summary()
```

#### Inference diagnostics

```{r}
plot(fit.brm)
```

Let's make our own trace plot 

```{r}
fit.brm %>% 
  spread_draws(b_Intercept) %>% 
  clean_names() %>% 
  mutate(chain = as.factor(chain)) %>% 
  ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + 
  geom_line()
```

##### When things go wrong 

```{r}
df.data = tibble(y = c(-1, 1))

fit.brm1 = brm(
  data = df.data,
  family = gaussian,
  formula = y ~ 1,
  prior = c(
    prior(uniform(-1e10, 1e10), class = Intercept),
    prior(uniform(0, 1e10), class = sigma)
  ),
  inits = list(
    list(Intercept = 0, sigma = 1),
    list(Intercept = 0, sigma = 1)
  ),
  iter = 4000,
  warmup = 1000,
  chains = 2,
  file = "fit_brm1"
)
```

Let's take a look at the posterior distributions of the model parameters: 

```{r}
summary(fit.brm1)
```

Not looking good -- The estimates and credible intervals are off the charts. And the effective samples sizes in the chains are very small. 

Let's visualize the trace plots:

```{r}
fit.brm1 %>% 
  plot()
```

```{r}
fit.brm1 %>% 
  spread_draws(b_Intercept) %>% 
  clean_names() %>% 
  mutate(chain = as.factor(chain)) %>% 
  ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + 
  geom_line()
```

Given that we have so little data in this case, we need to help the model a little bit by providing some slighlty more specific priors. 

```{r}
fit.brm2 = brm(
  data = df.data,
  family = gaussian,
  formula = y ~ 1,
  prior = c(
    prior(normal(0, 10), class = Intercept), # more reasonable priors
    prior(cauchy(0, 1), class = sigma)
  ),
  iter = 4000,
  warmup = 1000,
  chains = 2,
  seed = 1,
  file = "fit_brm2"
)
```

Let's take a look at the posterior distributions of the model parameters: 

```{r}
fit.brm2 %>% 
  summary()
```

This looks much better. There is still some uncertainty over the estimate, but it has reduced dramatically. 

Let's visualize the trace plots:

```{r}
fit.brm2 %>% 
  plot()
```

```{r}
fit.brm2 %>% 
  spread_draws(b_Intercept) %>% 
  clean_names() %>% 
  mutate(chain = as.factor(chain)) %>% 
  ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + 
  geom_line()
```

Looking mostly good -- except for one hiccup on sigma ... 


### Dealing with heteroscedasticity 

```{r}
# make example reproducible 
set.seed(0)

df.variance = tibble(
  group = rep(c("3yo", "5yo", "adults"), each = 20),
  response = rnorm(60, mean = rep(c(0, 5, 8), each = 20), sd = rep(c(3, 1.5, 0.3), each = 20))
)

df.variance %>%
  ggplot(aes(x = group, y = response)) +
  geom_jitter(height = 0,
              width = 0.1,
              alpha = 0.7)
```

```{r, cache=TRUE}
fit.variance = brm(
  formula = bf(response ~ group,
               sigma ~ group),
  data = df.variance,
  file = "variance"
)
```

```{r}
fit.variance %>% 
  summary()
```


Visualize the results 

```{r}
grid = df.variance %>%
  data_grid(group)

fits = grid %>%
  add_fitted_draws(fit.variance)

preds = grid %>%
  add_predicted_draws(fit.variance)

df.variance %>%
  ggplot(aes(x = response, y = group)) +
  geom_halfeyeh(aes(x = .value), relative_scale = 0.7, position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction), data = preds) +
  geom_point(data = df.variance) +
  scale_color_brewer()
```

```{r}
grid %>%
  add_fitted_draws(fit.variance, dpar = TRUE) %>%
  ggplot(aes(x = sigma, y = group)) +
  geom_halfeyeh() +
  geom_vline(xintercept = 0, linetype = "dashed")
```

### Ordinal regression 

```{r}
df.cars = mtcars %>% 
  mutate(cyl = ordered(cyl)) # creates an ordered factor
```


```{r}
df.cars %>% str()
```

```{r}
fit.cars = brm(formula = cyl ~ mpg,
               data = df.cars,
               family = "cumulative",
               file = "cars",
               seed = 1)
```

Visualize the results

```{r}
data_plot = df.cars %>%
  ggplot(aes(x = mpg, y = cyl, color = cyl)) +
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "cyl")

fit_plot = df.cars %>%
  data_grid(mpg = seq_range(mpg, n = 101)) %>%
  add_fitted_draws(fit.cars, value = "P(cyl | mpg)", category = "cyl") %>%
  ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) +
  stat_lineribbon(aes(fill = cyl),
                  alpha = 1/5,
                  .width = c(0.95)) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2")

plot_grid(ncol = 1, align = "v",
  data_plot,
  fit_plot
)
```

Posterior predictive check: 

```{r}
df.cars %>%
  select(mpg) %>%
  add_predicted_draws(fit.cars, prediction = "cyl", seed = 1234) %>%
  ggplot(aes(x = mpg, y = cyl)) +
  geom_count(color = "gray75") +
  geom_point(aes(fill = cyl),
    data = df.cars,
    shape = 21,
    size = 2) +
  scale_fill_brewer(palette = "Dark2") +
  geom_label_repel(
    data = . %>% ungroup() %>% filter(cyl == "8") %>% filter(mpg == max(mpg)) %>% dplyr::slice(1),
    label = "posterior predictions",
    xlim = c(26, NA),
    ylim = c(NA, 2.8),
    point.padding = 0.3,
    label.size = NA,
    color = "gray50",
    segment.color = "gray75") +
  geom_label_repel(
    data = df.cars %>% filter(cyl == "6") %>% filter(mpg == max(mpg)) %>% dplyr::slice(1),
    label = "observed data",
    xlim = c(26, NA),
    ylim = c(2.2, NA),
    point.padding = 0.2,
    label.size = NA,
    segment.color = "gray35")
```




## Playing around with BRMS 

### Simulate data 

```{r}
# make example reproducible 
set.seed(1)

b0 = 1
b1 = 2
b2 = 3
sample_size = 100 
sd_residual = 2

df.data = tibble(
  x1 = runif(sample_size, min = 0, max = 1),
  x2 = runif(sample_size, min = 0, max = 1),
  y = b0 + b1 * x1 + b2 * x2 + rnorm(sample_size, sd = sd_residual)
)

```

### Fit linear model 

```{r}
fit.lm = lm(formula = y ~ 1 + x1 + x2,
             data = df.data)

fit.lm %>% summary()
```

### Fit Bayesian regression 

```{r, cache=TRUE}

fit.brm = brm(formula = y ~ 1 + x1 + x2,
              data = df.data,
              file = "fit_brm")

fit.brm %>% summary()
```

#### Inspecting the stan code 

```{r}
fit.brm %>% stancode()
```

One thing worth noticing: by default, "brms" centers the predictors which makes it easier to assign a default prior to the intercept. 

#### Getting the priors

Notice that we didn't specify any priors in the model. By default, "brms" assigns weakly informative priors to the parameters in the model. We can see what these are by running the following command: 

```{r}
fit.brm %>% 
  prior_summary()
```

We can also get information about which priors need to be specified before fitting a model:

```{r}
get_prior(formula = y ~ 1 + x1 + x2,
          family = "gaussian",
          data = df.data)
```

#### Setting the priors

```{r}
fit.brm2 = brm(
  formula = y ~ 1 + x1 + x2,
  family = "gaussian",
  data = df.data,
  prior = c(
    prior(normal(0, 10), class = "b", coef = "x1"),
    prior(normal(0, 20), class = "b", coef = "x2"),
    prior(student_t(3, 3, 10), class = "Intercept"),
    prior(student_t(3, 0, 10), class = "sigma")
  ),
  inits = list(
    list(Intercept = 0, sigma = 1, x1 = 0, x2 = 0),
    list(Intercept = 0, sigma = 1, x1 = 0, x2 = 0),
    list(Intercept = 0, sigma = 1, x1 = 0, x2 = 0),
    list(Intercept = 1, sigma = 2, x1 = 2, x2 = -2)
  ),
  iter = 4000,
  warmup = 1000,
  chains = 4,
  file = "brm2",
  seed = 1
)

fit.brm2 %>% summary()
```

#### Hypothesis test 

```{r}
hypothesis(fit.brm,
           hypothesis = "x1 > 0")
```

Test by hand

```{r}
fit.brm %>% 
  gather_draws(b_x1) %>% 
  clean_names() %>% 
  summarize(p_value = 1 - (sum(value > 0) / n()))
```

Easy to specify contrasts and answer research questions in a very flexible way. 


#### Exploring the posterior

##### Pairs plot 

```{r}
fit.brm %>% 
  posterior_samples() %>% 
  select(b_Intercept:sigma) %>% 
  ggpairs(lower = list(continuous = wrap("points", alpha = 0.03)),
          upper = list(continuous = wrap("cor", size = 8))) + 
  theme(panel.grid.major = element_blank())
```

##### Densities and trace plot 

```{r}
fit.brm %>% plot()
```

Using the "bayesplot" package

```{r}
df.samples = posterior_samples(fit.brm, add_chain = T)

df.samples %>% 
  select(b_Intercept:sigma, chain) %>% 
  mcmc_trace(facet_args = list(ncol = 4))
```

##### Autocorrelation plot

```{r}
df.samples %>% 
mcmc_acf(pars = fit.brm %>% get_variables() %>% .[1:4],
         lags = 4)
```

Looking good! The autocorrelation should become very small as the lag increases (indicating that we are getting independent samples from the posterior). 



##### Posterior predictive fit 

Draw samples from the posterior and see whether it looks like the actual data. 

```{r}
pp_check(fit.brm, nsamples = 100)
```

Do this by hand: 

```{r}
# get the variable names in the model 
get_variables(fit.brm)
```

```{r}
# let's get the posterior samples 
df.samples = fit.brm %>% 
  spread_draws(b_Intercept, b_x1, b_x2, sigma) %>%
  # gather_draws(b_Intercept, b_x1, b_x2, sigma) %>% 
  clean_names()

# generate predictions 
df.predictions = df.samples %>%
  sample_n(size = 100) %>%
  rowwise() %>%
  mutate(prediction = list(b_intercept + b_x1 * df.data$x1 + b_x2 * df.data$x2 + rnorm(n = nrow(df.data), sd = sigma))) %>%
  unnest(prediction, .id = "sample")

# plot posterior predictive 
df.data %>% 
  ggplot(data = .,
         mapping = aes(x = y)) +
  stat_density(geom = "line",
               size = 1.5) +
  stat_density(data = df.predictions,
               mapping = aes(x = prediction,
                             group = draw),
               geom = "line",
               position = "identity",
               color = "lightblue",
               alpha = 0.2)

```

##### Plotting point summaries and intervals 

Intervals 

```{r}
fit.brm %>% 
  gather_draws(b_Intercept, b_x1, b_x2, sigma) %>% 
  median_qi() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = variable, 
                       y = value,
                       ymin = lower,
                       ymax = upper)) + 
  geom_pointrange() +
  coord_flip()
```

Intervals with densities

```{r}
fit.brm %>%
  gather_draws(b_Intercept, b_x1, b_x2, sigma) %>%
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(y = variable, x = value)) +
  geom_halfeyeh()
```


##### Fit / prediction curves

Fitted intervals 

```{r}
df.data %>% 
  data_grid(x1 = seq_range(x1, n = 10),
            x2 = seq_range(x2, n = 10)) %>% 
  add_fitted_draws(fit.brm) %>% 
  clean_names() %>% 
  ggplot(aes(x = x1, y = y)) +
  stat_lineribbon(aes(y = value)) + 
  geom_point(data = df.data) +
  scale_fill_brewer(palette = "Greys")
```

Prediction intervals 

```{r}
df.data %>% 
  data_grid(x1 = seq_range(x1, n = 10),
            x2 = seq_range(x2, n = 10)) %>% 
  add_predicted_draws(fit.brm) %>% 
  clean_names() %>% 
  ggplot(aes(x = x1, y = y)) +
  stat_lineribbon(aes(y = prediction)) + 
  geom_point(data = df.data) +
  scale_fill_brewer(palette = "Greys")
```

Show fits as individuals lines 

```{r}
df.data %>% 
  data_grid(x1 = seq_range(x1, n = 10),
            x2 = seq_range(x2, n = 10)) %>%
  add_fitted_draws(fit.brm, n = 10) %>%
  clean_names() %>%
  # arrange(.draw)
  ggplot(aes(x = x1, y = y)) +
  geom_line(aes(y = value, group = paste(x2, draw)), alpha = .1) +
  geom_point(data = df.data) +
  scale_color_brewer(palette = "Dark2")
```



## Comparing hypotheses 

### Bayes factor 

Specify one model with, and one model without the predictor and then use the `bayes_factor()` function from "brms". 

```{r}
# model with the treatment effect
fit1 <- brm(
  formula = count ~ log_Age_c + log_Base4_c + Trt_c,
  data = epilepsy, family = negbinomial(), 
  prior = prior(normal(0, 1), class = b),
  file = "fit1",
  save_all_pars = TRUE
)
summary(fit1)

# model without the treatment effect
fit2 <- brm(
  count ~ log_Age_c + log_Base4_c,
  data = epilepsy, family = negbinomial(), 
  prior = prior(normal(0, 1), class = b),
  file = "fit2",
  save_all_pars = TRUE
)
summary(fit2)

# compute the bayes factor
bayes_factor(fit1, fit2)
```

### Posterior over a parameter



```{r}
fit1 %>%
  gather_draws(b_Intercept, b_log_Age_c, b_log_Base4_c, b_Trt_c, shape) %>% 
  mode_hdi() %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(x = variable, 
                       y = value,
                       ymin = lower,
                       ymax = upper)) + 
  geom_hline(yintercept = 0,
             color = "red") +
  geom_pointrange() + 
  coord_flip()
```


## Dealing with heteroscedasticity 

```{r}
set.seed(1234)

df.variance = tibble(
  group = rep(c("a", "b"), each = 20),
  response = rnorm(40, mean = rep(c(1, 3), each = 20), sd = rep(c(1, 3), each = 20))
)

df.variance %>%
  ggplot(aes(x = group, y = response)) +
  geom_point()
```

```{r, cache=TRUE}
fit.variance = brm(
  formula = bf(response ~ group,
               sigma ~ group),
  family = "gaussian", 
  data = df.variance,
  file = "variance"
)
```

Visualize the results 

```{r}
grid = df.variance %>%
  data_grid(group)

fits = grid %>%
  add_fitted_draws(fit.variance)

preds = grid %>%
  add_predicted_draws(fit.variance)

df.variance %>%
  ggplot(aes(x = response, y = group)) +
  geom_halfeyeh(aes(x = .value), relative_scale = 0.7, position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction), data = preds) +
  geom_point(data = df.variance) +
  scale_color_brewer()
```

```{r}
grid %>%
  add_fitted_draws(fit.variance, dpar = TRUE) %>%
  ggplot(aes(x = sigma, y = group)) +
  geom_halfeyeh() +
  geom_vline(xintercept = 0, linetype = "dashed")
```

How different are the groups? 

```{r}
fit.variance %>% 
  gather_draws(b_Intercept, b_sigma_Intercept, b_groupb, b_sigma_groupb) %>% 
  clean_names() %>% 
  ggplot(data = .,
         mapping = aes(y = variable, x = value)) +
  geom_halfeyeh()
```

Linear model 

```{r}
fit = lm(formula = response ~ 1 + group,
         data = df.variance)

fit %>% summary()
```

## Summarizing the posterior 

Quantile interval vs. highest density interval 

```{r}
# make example reproducible
set.seed(1)

# generate data 
df.multimodal = tibble(
  x = c(rnorm(5000, 0, 1), rnorm(2500, 4, 1))
)

```

Visualize: 

```{r}
df.multimodal %>%
  ggplot(aes(x = x)) +
  stat_density(fill = "gray75",
               color = "black") +
  stat_pointintervalh(aes(y = -0.025), point_interval = mode_hdi, .width = c(.95, .80)) +
  annotate("text", label = "mode, 80% and 95% highest-density intervals", x = 6, y = -0.025, hjust = 0, vjust = 0.3) +
  stat_pointintervalh(aes(y = -0.05), point_interval = median_qi, .width = c(.95, .80)) +
  annotate("text", label = "median, 80% and 95% quantile intervals", x = 6, y = -0.05, hjust = 0, vjust = 0.3) +
  stat_pointintervalh(aes(y = -0.075), point_interval = mode_hdci, .width = c(.95, .80)) +
  annotate("text", label = "mode, 80% and 95% continuous highest-density intervals", x = 6, y = -0.075, hjust = 0, vjust = 0.3) +
  xlim(-3.5, 16)
```

## Simulate linear and Bayesian regression 

```{r}
# make example reproducible 
set.seed(1)

# simulate data
n = 10
df.conditions = tibble(condition = rep(c("A","B","C","D","E"), n),
                       response = rnorm(n * 5, c(0,1,2,1,-1), 0.5))

```

Fit linear model: 

```{r}
fit.lm1 = lm(formula = response ~ 1 + condition,
             data = df.conditions)

fit.lm1 %>% summary()
```

Fit Bayesian model: 

```{r, cache=TRUE}
fit.brm1 = brm(formula = response ~ 1 + condition,
               data = df.conditions)
```




## Scratch pad 

```{r}
data(rugged, package = "rethinking")
```



## Additional resources 

- [Tutorial on visualizing brms posteriors with tidybayes](https://mjskay.github.io/tidybayes/articles/tidy-brms.html)
- [Hypothetical outcome plots](https://mucollective.northwestern.edu/files/2018-HOPsTrends-InfoVis.pdf)
- [Visual MCMC diagnostics](https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html#general-mcmc-diagnostics)

## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r session}
sessionInfo()
```

## References


