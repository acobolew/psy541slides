---
title: "Class 22"
author: "Tobias Gerstenberg"
date: ""
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
    pandoc_args: ["--number-offset=21"]
---

```{r, eval=FALSE, echo=FALSE, include=FALSE}
install.packages(c("tidybayes", "extraDistr"))
devtools::install_github("greta-dev/greta")
```

# Bayesian data analysis 2

## Learning goals 

- Simple Bayesian inference example. 
- Bayes' rule in action. 
  - Common likelihood functions. 
  - Common prior functions.
  - How to do inference.
- Doing Bayesian data analysis with `greta`
  - A simple linear regression.

## Load packages and set plotting theme  

```{r, message=FALSE}
library("knitr")      # for knitting RMarkdown 
library("janitor")    # for cleaning column names
library("greta")      # for writing Bayesian models
library("tidybayes")  # tidying up results from Bayesian models
library("gganimate")  # for animations
library("patchwork")  # for figure panels 
library("extraDistr") # additional probability distributions
library("tidyverse")  # for wrangling, plotting, etc. 
```

```{r}
theme_set(theme_classic() + #set the theme 
    theme(text = element_text(size = 20))) #set the default text size

# these options here change the formatting of how comments are rendered
opts_chunk$set(comment = "",
               fig.show = "hold")
```

## Distributions 

### Normal vs Student-t distribution

```{r, fig.cap="Comparison between the normal distribution and the student-t distribution."}
tibble(x = c(-5, 5)) %>% 
  ggplot(aes(x = x)) + 
  stat_function(fun = "dnorm",
                size = 1,
                color = "blue") +
  stat_function(fun = "dt",
                size = 1,
                color = "red",
                args = list(df = 1))
```

### Beta distributions

```{r, fig.cap="Beta distributions", fig.height=4, fig.width=8}

fun.draw_beta = function(shape1, shape2){
  ggplot(data = tibble(x = c(0, 1)),
         aes(x = x)) + 
  stat_function(fun = "dbeta",
                size = 1,
                color = "black",
                args = list(shape1 = shape1, shape2 = shape2)) +
    annotate(geom = "text", 
             label = str_c("Beta(", shape1,",",shape2,")"),
             x = 0.5,
             y = Inf,
             hjust = 0.5,
             vjust = 0.2,
             size = 4) +
    scale_x_continuous(breaks = seq(0, 1, 0.2)) +
    coord_cartesian(clip = "off") + 
    theme(axis.title.x = element_blank(),
          axis.text.x = element_text(size = 12),
          plot.margin = margin(b = 0.6, t = 0.2, r = 0.1, unit = "cm"))
}

shape1 = c(1, 0.5, 5, 1, 8, 20)
shape2 = c(1, 0.5, 5, 9, 2, 20)

p.list = map2(.x = shape1, .y = shape2, ~ fun.draw_beta(.x, .y))

wrap_plots(p.list, ncol = 3)
```

### Normal distributions 

```{r, fig.cap="Normal distributions with different standard deviation."}
tibble(x = c(-10, 10)) %>% 
  ggplot(aes(x = x)) + 
  stat_function(fun = "dnorm",
                size = 1,
                color = "blue",
                args = list(sd = 1)) +
  stat_function(fun = "dnorm",
                size = 1,
                color = "red",
                args = list(sd = 5))
```

### Distributions for non-negative parameters 

```{r, fig.cap="Cauchy and Gamma distribution."}
tibble(x = c(0, 10)) %>% 
  ggplot(aes(x = x)) + 
  stat_function(fun = "dcauchy",
                size = 1,
                color = "blue",
                args = list(location = 0, scale = 1),
                xlim = c(0, 10)) +
  stat_function(fun = "dgamma",
                size = 1,
                color = "red",
                args = list(shape = 4, rate = 2))
```


## Inference via sampling 

Example for how we can compute probabilities based on random samples generated from a distribution. 

```{r}
# generate samples 
df.samples = tibble(x = rnorm(n = 10000, mean = 1, sd = 2)) 

# visualize distribution 
ggplot(data = df.samples,
       mapping = aes(x = x)) + 
  stat_density(geom = "line",
               color = "red",
               size = 2) + 
  stat_function(fun = "dnorm",
                args = list(mean = 1, sd = 2),
                color = "black",
                linetype = 2)

# calculate probability based on samples 
df.samples %>% 
  summarize(prob = sum(x >= 0 & x < 4)/n())

# calculate probability based on theoretical distribution
pnorm(4, mean = 1, sd = 2) - pnorm(0, mean = 1, sd = 2)
```

## Greta 

You can find out more about how get started with "greta" here: [https://greta-stats.org/articles/get_started.html](https://greta-stats.org/articles/get_started.html). Make sure to install the development version of "greta" (as shown in the "install-packages" code chunk above: `devtools::install_github("greta-dev/greta")`).

### Attitude data set 

```{r}
# load the attitude data set 
df.attitude = attitude
```

Visualize relationship between how well complaints are handled and the overall rating of an employee

```{r}
ggplot(data = df.attitude,
       mapping = aes(x = complaints,
                     y = rating)) +
  geom_point()
```

### Frequentist analysis 

```{r}
# fit model 
fit = lm(formula = rating ~ 1 + complaints, 
         data = df.attitude)

# print summary
fit %>% summary()
```

Visualize the model's predictions

```{r}
ggplot(data = df.attitude,
       mapping = aes(x = complaints,
                     y = rating)) +
  geom_smooth(method = "lm",
              color = "black") + 
  geom_point()
```

### Bayesian regression

#### Fit the model

```{r}
# variables & priors
b0 = normal(0, 10)
b1 = normal(0, 10)
sd = cauchy(0, 3, truncation = c(0, Inf))

# linear predictor
mu = b0 + b1 * df.attitude$complaints

# observation model (likelihood)
distribution(df.attitude$rating) = normal(mu, sd)

# define the model
m = model(b0, b1, sd)
```

Visualize the model as graph: 

```{r}
# plotting
plot(m)
```

Draw samples from the posterior distribution: 

```{r, message=FALSE, cache=TRUE}
# sampling
draws = mcmc(m, n_samples = 1000)

# tidy up the draws
df.draws = tidy_draws(draws) %>% 
  clean_names()
```

#### Visualize the priors

These are the priors I used for the intercept, regression weights, and the standard deviation of the Gaussian likelihood function:  

```{r}
# Gaussian
ggplot(tibble(x = c(-30, 30)),
       aes(x = x)) +
  stat_function(fun = "dnorm", 
                size = 2,
                args = list(sd = 10))

# Cauchy
ggplot(tibble(x = c(0, 30)),
       aes(x = x)) +
  stat_function(fun = "dcauchy", 
                size = 2,
                args = list(location = 0,
                            scale = 3))
```

#### Visualize the posteriors

This is what the posterior looks like for the three parameters in the model: 

```{r}
df.draws %>% 
  select(draw:sd) %>% 
  pivot_longer(cols = -draw,
               names_to = "index",
               values_to = "value") %>% 
  ggplot(data = .,
         mapping = aes(x = value)) + 
  stat_density(geom = "line") + 
  facet_grid(rows = vars(index),
             scales = "free_y",
             switch = "y") + 
  annotate("segment", x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + 
  annotate("segment", x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + 
  theme(legend.position = "none",
        strip.background = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 10),
        axis.line = element_blank(),
        strip.text.x = element_blank())
```

#### Visualize model predictions 

Let's take some samples from the posterior to visualize the model predictions: 

```{r}
ggplot(data = df.attitude,
       mapping = aes(x = complaints, 
                     y = rating)) + 
  geom_abline(data = df.draws %>% 
                sample_n(size = 50),
              aes(intercept = b0, 
                  slope = b1),
              alpha = 0.3,
              color = "lightblue") + 
  geom_point() 
```

#### Posterior predictive check 

Let's make an animation that illustrates what predicted data sets (based on samples from the posterior) would look like: 

```{r, message=FALSE}
p = df.draws %>% 
  sample_n(size = 10) %>%  
  mutate(complaints = list(seq(min(df.attitude$complaints),
                 max(df.attitude$complaints),
                 length.out = nrow(df.attitude)))) %>% 
  unnest(c(complaints)) %>% 
  mutate(prediction = b0 + b1 * complaints + rnorm(n(), sd = sd)) %>% 
  ggplot(aes(x = complaints, y = prediction)) + 
  geom_point(alpha = 0.8,
             color = "lightblue") +
  geom_point(data = df.attitude,
             aes(y = rating,
                 x = complaints)) +
  coord_cartesian(xlim = c(20, 100),
                  ylim = c(20, 100)) +
  transition_manual(draw)

animate(p, nframes = 60, width = 800, height = 600, res = 96, type = "cairo")

# anim_save("posterior_predictive.gif")
```

#### Prior predictive check 

And let's illustrate what data we would have expected to see just based on the information that we encoded in our priors. 

```{r, message=FALSE}
sample_size = 10

p = tibble(b0 = rnorm(sample_size, mean = 0, sd = 10),
           b1 = rnorm(sample_size, mean = 0, sd = 10),
           sd = rhcauchy(sample_size, sigma = 3),
           draw = 1:sample_size) %>% 
  mutate(complaints = list(runif(nrow(df.attitude),
                                 min = min(df.attitude$complaints),
                                 max = max(df.attitude$complaints)))) %>% 
  unnest(c(complaints)) %>% 
  mutate(prediction = b0 + b1 * complaints + rnorm(n(), sd = sd)) %>% 
  ggplot(aes(x = complaints, y = prediction)) + 
  geom_point(alpha = 0.8,
             color = "lightblue") +
  geom_point(data = df.attitude,
             aes(y = rating,
                 x = complaints)) +
  transition_manual(draw)

animate(p, nframes = 60, width = 800, height = 600, res = 96, type = "cairo")

# anim_save("prior_predictive.gif")
```

## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r}
sessionInfo()
```

