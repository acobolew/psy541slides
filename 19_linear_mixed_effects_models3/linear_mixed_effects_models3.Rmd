---
title: "Class 19"
author: "Tobias Gerstenberg"
date: "March 1st, 2021"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
    pandoc_args: ["--number-offset=18"]
bibliography: [packages.bib]
nocite: '@*'
---

# Linear mixed effects models 3

## Learning goals 

- Pitfalls in fitting `lmers()`s (and what to do about it). 
- Understanding `lmer()` syntax even better.

## Load packages and set plotting theme  

```{r, message=FALSE}
library("knitr")       # for knitting RMarkdown 
library("kableExtra")  # for making nice tables
library("janitor")     # for cleaning column names
library("broom.mixed") # for tidying up linear mixed effects models 
library("patchwork")   # for making figure panels
library("lme4")        # for linear mixed effects models
library("afex")        # for ANOVAs
library("car")         # for ANOVAs
library("datarium")    # for ANOVA dataset
library("modelr")      # for bootstrapping
library("boot")        # also for bootstrapping
library("ggeffects")   # for plotting marginal effects
library("emmeans")     # for marginal effects
library("tidyverse")   # for wrangling, plotting, etc. 
```

```{r}
theme_set(theme_classic() + #set the theme 
            theme(text = element_text(size = 20))) #set the default text size

# knitr display options 
opts_chunk$set(comment = "",
               fig.show = "hold")

# include references for used packages
write_bib(.packages(), "packages.bib") 


# set contrasts to using sum contrasts
options(contrasts = c("contr.sum","contr.poly"))

# suppress grouping warning messages
options(dplyr.summarise.inform = F)
```

## Load data set 

```{r}
# load sleepstudy data set 
df.sleep = sleepstudy %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(subject = as.character(subject)) %>% 
  select(subject, days, reaction)
```

```{r}
# add two fake participants (with missing data)
df.sleep = df.sleep %>% 
  bind_rows(tibble(subject = "374",
                   days = 0:1,
                   reaction = c(286, 288)),
            tibble(subject = "373",
                   days = 0,
                   reaction = 245))
```

## Understanding the lmer() syntax 

Here is an overview of how to specify different kinds of linear mixed effects models.

```{r, echo=F}
tibble(formula = c("`dv ~ x1 + (1 | g)`",
                   "`dv ~ x1 + (0 + x1 | g)`",
                   "`dv ~ x1 + (x1 | g)`",
                   "`dv ~ x1 + (x1 || g)`",
                   "`dv ~ x1 + (1 | school) + (1 | teacher)`",
                   "`dv ~ x1 + (1 | school/teacher)`"),
       description = c("Random intercept for each level of `g`",
                       "Random slope for each level of `g`",
                       "Correlated random slope and intercept for each level of `g`",
                       "Uncorrelated random slope and intercept for each level of `g`",
                       "Random intercept for each level of `school` and for each level of `teacher` (crossed)",
                       "Random intercept for each level of `school` and for each level of `teacher` in `school` (nested)")) %>% 
  kable()
```

Note that this `(1 | school/teacher)` is equivalent to `(1 | school) + (1 | teacher:school)` (see [here](https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified)). 

### Between subjects ANOVA 

Let's start with a between subjects ANOVA (which means we are in `lm()` world.)

```{r}
df.reasoning = sk2011.1

aov_ez(id = "id",
       dv = "response",
       between = "instruction",
       data = df.reasoning)

lm(formula = response ~ instruction,
   data = df.reasoning %>% 
     group_by(id, instruction) %>% 
     summarize(response = mean(response)) %>% 
     ungroup()) %>% 
  Anova(type = 3,
        method = "F")
```


### Repeated-measures ANOVA 

```{r}
df.reasoning_within = sk2011.1 %>% 
  filter(instruction == "probabilistic")
  
aov_ez(id = "id",
       dv = "response",
       within = c("validity", "plausibility"),
       data = df.reasoning_within)

mixed(formula = response ~ validity * plausibility + (1 | id) + (1 | validity:id) + (1 | plausibility:id),
      data = df.reasoning_within %>%
        group_by(id, validity, plausibility) %>%
        summarize(response = mean(response)))
```

### Mixed ANOVA 

```{r}
aov_ez(id = "id",
       dv = "response",
       between = "instruction",
       within = c("validity", "plausibility"),
       data = df.reasoning)

mixed(formula = response ~ instruction * validity * plausibility + (1 | id) + (1 | validity:id) + (1 | plausibility:id),
      data = df.reasoning %>%
        group_by(id, validity, plausibility, instruction) %>%
        summarize(response = mean(response)))


lmer(formula = response ~ instruction * validity * plausibility + (1 | id) + (1 | validity:id) + (1 | plausibility:id),
      data = df.reasoning %>%
        group_by(id, validity, plausibility, instruction) %>%
        summarize(response = mean(response))) %>% 
  summary()
```


#### Reaction time data 

```{r}
df.reaction = md_12.1

df.reaction.filtered = md_12.1 %>% 
  filter(angle %in% c("0", "4"))

# 1. single predictor with three levels: same results
aov_ez(id = "id",
       dv = "rt",
       within = c("angle"),
       data = df.reaction)

lmer(formula = rt ~ 1 + angle + (1 | id),
     data = df.reaction %>% 
       group_by(id, angle) %>% 
       summarize(rt = mean(rt)) %>% 
       ungroup()) %>%
  Anova(type = 3,
        test.statistic = "F")


# 2. two predictors, angle with only two levels: same results 
aov_ez(id = "id",
       dv = "rt",
       within = c("noise", "angle"),
       data = df.reaction.filtered)

lmer(formula = rt ~ 1 + noise * angle  + (1 | id) + (1 | noise:id) + (1 | angle:id),
     data = df.reaction.filtered) %>%
  Anova(type = 3,
        test.statistic = "F")

# 3. two predictors, angle with THREE two levels: DIFFERENT results
aov_ez(id = "id",
       dv = "rt",
       within = c("noise", "angle"),
       data = df.reaction)

mixed(formula = rt ~ 1 + noise * angle + (1 | id) + (1 | noise:id) + (1 | angle:id),
      data = df.reaction)

mixed(formula = rt ~ 1 + noise * angle + (1 | id) + (0 + noise | id) + (0 + angle | id),
      data = df.reaction)
```

```{r}
mixed(formula = rt ~ noise * angle + (noise + angle | id),
      data = df.reaction)

lmer(formula = rt ~ noise * angle + (noise + angle | id),
      data = df.reaction) %>% 
  Anova(type = 3,
        test.statistic = "F")
```



#### Another data set 

```{r, eval=FALSE}
df.intelligence = md_15.1

aov_ez(id = "id",
       dv = "iq",
       within = c("timecat"),
       data = df.intelligence,
       print.formula = T)

aov_4(formula = iq ~ 1 + (timecat | id),
        data = df.intelligence)

lmer(formula = iq ~ 1 + timecat  + (1 | id),
     data = df.intelligence) %>%
  Anova(type = 3,
        test.statistic = "F")
```

#### Final data set 

```{r}
df.naming = fhch2010

df.naming = df.naming %>% 
  filter(id %in% str_c(rep(c("N", "L"), each = 5), 1:4))
  
aov_ez(id = "id",
       dv = "log_rt",
       within = c("stimulus", "density", "frequency"),
       data = df.naming,
       print.formula = T)

mixed(formula = log_rt ~ 1 + stimulus * density * frequency + (1 | id) + 
        (1 | stimulus:id) + (1 | density:id) + (1 | frequency:id) + 
        (1 | stimulus:density:id) + (1 | stimulus:frequency:id) + (1 | density:frequency:id),
      data = df.naming %>% 
        group_by(id, task, stimulus, density, frequency) %>% 
        summarize(log_rt = mean(log_rt)) %>% 
        ungroup())

# aov_ez(id = "id",
#        dv = "log_rt",
#        within = c("stimulus","density", "frequency", "length"),
#        between = c("task"),
#        data = df.naming,
#        print.formula = T)
# 
# aov_4(formula = log_rt ~ task + (stimulus * density * frequency * length | id),
#         data = df.naming)
# 
# lmer(formula = log_rt ~ 1 + task + stimulus * density * frequency * length + (stimulus + density + frequency + length | id),
#      data = df.naming) %>% 
# Anova(type = 3,
#       test.statistic = "F")
```

```{r, eval=FALSE}
df.naming = fhch2010

aov_ez(id = "id",
       dv = "log_rt",
       within = c("stimulus"),
       between = c("task"),
       data = df.naming,
       print.formula = T)

aov_4(formula = log_rt ~ task + (stimulus | id),
        data = df.naming)

lmer(formula = log_rt ~ 1 + task * stimulus + (1 | id),
     data = df.naming) %>% 
Anova(type = 3,
      test.statistic = "F")



```

### Mixed ANOVA

```{r}
set.seed(123)
data("weightloss", package = "datarium")

# Modify it to have three-way mixed design
df.weightloss = weightloss %>%
  mutate(id = rep(1:24, 2)) %>% 
  pivot_longer(cols = t1:t3,
               names_to = "timepoint",
               values_to = "score") %>% 
  arrange(id)

fit = aov_ez(id = "id",
       dv = "score",
       between = "exercises",
       within = c("diet", "timepoint"),
       data = df.weightloss)

contrasts = list(first_two_vs_last = c(-0.5, -0.5, 1),
                 linear_increase = c(-1, 0, 1))

fit %>% 
  emmeans(spec = "timepoint",
          contr = contrasts)

ggplot(data = df.weightloss,
       mapping = aes(x = timepoint,
                     y = score,
                     group = diet,
                     color = diet)) + 
  geom_point(position = position_jitterdodge(dodge.width = 0.5,
                                  jitter.width = 0.1,
                                  jitter.height = 0),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot",
               position = position_dodge(width = 0.5)) +
  facet_wrap(~ exercises) + 
  scale_color_brewer(palette = "Set1")


ggplot(data = df.weightloss,
       mapping = aes(x = timepoint,
                     y = score)) + 
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot") +
  scale_color_brewer(palette = "Set1")

```

### Lmer (mixed)

```{r, message=FALSE}
df.politeness = read_csv("data/politeness_data.csv") %>% 
  mutate(scenario = as.factor(scenario))

fit %>% 
  summary()

fit = mixed(formula = frequency ~ 1 + attitude * gender + (1 | subject) + (1 | scenario),
      data = df.politeness)

fit %>% 
  emmeans(specs = pairwise ~ attitude + gender,
          adjust = "none")


aov_ez(id = "subject",
       dv = "frequency",
       between = "gender",
       within = "attitude",
       data = df.politeness)


df.politeness %>% 
  kable(digits = 2) %>% 
  kable_styling(bootstrap_options = "striped",
                full_width = F)


ggplot(data = df.politeness,
        mapping = aes(x = attitude,
                      y = frequency,
                      group = gender,
                      color = gender)) + 
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot") + 
  scale_color_brewer(palette = "Set1")


ggplot(data = df.politeness,
        mapping = aes(x = scenario,
                      y = frequency)) + 
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot") + 
  scale_color_brewer(palette = "Set1")

ggplot(data = df.politeness,
        mapping = aes(x = subject,
                      y = frequency)) + 
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot") + 
  scale_color_brewer(palette = "Set1")
```



## Follow-up tests with emmeans

```{r}
fit = lmer(formula = reaction ~ 1 + days + (1 | subject),
           data = df.sleep %>% 
             mutate(days = as.factor(days)))

fit %>% 
  summary()

contrast = list(first_vs_second = c(-1, 1, rep(0, 8)),
                early_vs_late = c(rep(-1, 5)/5, rep(1, 5)/5))

fit %>% 
  emmeans(specs = "days",
          contr = contrast) %>% 
  pluck("contrasts")


fit = mixed(formula = reaction ~ 1 + days + (1 | subject),
           data = df.sleep %>% 
             mutate(days = as.factor(days)))

contrast = list(first_vs_second = c(-1, 1, rep(0, 8)),
                early_vs_late = c(rep(-1, 5)/5, rep(1, 5)/5))

fit %>% 
  emmeans(specs = "days",
          contr = contrast) %>% 
  pluck("contrasts")

df.sleep %>% 
  filter(days %in% c(0, 1)) %>% 
  group_by(days) %>% 
  summarize(reaction = mean(reaction)) %>% 
  kable(digits = 2) %>% 
  kable_styling(bootstrap_options = "striped",
                full_width = F)

df.sleep %>% 
  mutate(index = ifelse(days %in% 0:4, "early", "late")) %>% 
  group_by(index) %>% 
  summarize(reaction = mean(reaction)) %>% 
  kable(digits = 2) %>% 
  kable_styling(bootstrap_options = "striped",
                full_width = F)

df.sleep %>% 
  kable(digits = 2) %>% 
  kable_styling(bootstrap_options = "striped",
                full_width = F)


ggplot(data = df.sleep %>% 
         mutate(days = as.factor(days)),
       mapping = aes(x = days,
                     y = reaction)) + 
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.1) + 
  stat_summary(fun.data = "mean_cl_boot")
```

```{r}
df.cake = cake

lmer(angle ~ recipe * temperature + (1|recipe:replicate),
     data = df.cake) %>% 
  Anova(type = 3)

lme4::grouseticks

```


## Mixtures of participants 

What if we have groups of participants who differ from each other? Let's generate data for which this is the case.

```{r}
# make example reproducible 
set.seed(1)

sample_size = 20
b0 = 1
b1 = 2
sd_residual = 0.5
sd_participant = 0.5
mean_group1 = 1
mean_group2 = 10

df.mixed = tibble(
  condition = rep(0:1, each = sample_size), 
  participant = rep(1:sample_size, 2)) %>% 
  group_by(participant) %>% 
  mutate(group = sample(1:2, size = 1),
         intercept = ifelse(group == 1,
                            rnorm(n(), mean = mean_group1, sd = sd_participant),
                            rnorm(n(), mean = mean_group2, sd = sd_participant))) %>% 
  group_by(condition) %>% 
  mutate(value = b0 + b1 * condition + intercept + rnorm(n(), sd = sd_residual)) %>% 
  ungroup %>% 
  mutate(condition = as.factor(condition),
         participant = as.factor(participant))
```

#### Ignoring mixture

Let' first fit a model that ignores the fact that there are two different groups of participatns. 

```{r}
# fit model
fit.mixed = lmer(formula = value ~ 1 + condition + (1 | participant),
                data = df.mixed)

fit.mixed %>% summary()
```

Let's look at the model's predictions: 

```{r}
fit.mixed %>%
  augment() %>%
  clean_names() %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") +
  geom_line(aes(y = fitted),
             color = "red")
```


And let's simulate some data from the fitted model: 

```{r}
# simulated data 
fit.mixed %>%
  simulate() %>%
  bind_cols(df.mixed) %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = sim_1,
                       group = participant)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.5)
```

As we can see, the simulated data doesn't look like the data that was used to fit the model.  

#### Modeling mixture

Now, let's fit a model that takes the differences between groups into account by adding a fixed effect for `group`.

```{r}
# fit model
fit.grouped = lmer(formula = value ~ 1 + group + condition + (1 | participant),
                data = df.mixed)

fit.grouped %>% summary()
```

Note how the variance of the random intercepts is much smaller now that we've taken the group structure in the data into account. 

Let's visualize the model's predictions:

```{r}
fit.grouped %>%
  augment() %>%
  clean_names() %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") +
  geom_line(aes(y = fitted),
             color = "red")
```

And simulate some data from the model: 

```{r}
# simulated data 
fit.grouped %>%
  simulate() %>%
  bind_cols(df.mixed) %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = sim_1,
                       group = participant)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.5)
```

This time, the simulated data looks much more like the data that was used to fit the model. Yay! 


```{r}
ggpredict(model = fit.grouped,
          terms = "condition") %>% 
  plot()

ggpredict(model = fit.mixed,
          terms = "condition") %>% 
  plot()

# fit.mixed %>% 
#   summary()
# 
# fit.grouped %>% 
#   summary()
```


#### Heterogeneity in variance

The example above has shown that we can take overall differences between groups into account by adding a fixed effect. Can we also deal with heterogeneity in variance between groups? For example, what if the responses of one group exhibit much more variance than the responses of another group? 

Let's first generate some data with heterogeneous variance: 

```{r}
# make example reproducible 
set.seed(1)

sample_size = 20
b0 = 1
b1 = 2
sd_residual = 0.5
mean_group1 = 1
sd_group1 = 1
mean_group2 = 30
sd_group2 = 10

df.variance = tibble(
  condition = rep(0:1, each = sample_size), 
  participant = rep(1:sample_size, 2)) %>% 
  group_by(participant) %>% 
  mutate(group = sample(1:2, size = 1),
         intercept = ifelse(group == 1,
                            rnorm(n(), mean = mean_group1, sd = sd_group1),
                            rnorm(n(), mean = mean_group2, sd = sd_group2))) %>% 
  group_by(condition) %>% 
  mutate(value = b0 + b1 * condition + intercept + rnorm(n(), sd = sd_residual)) %>% 
  ungroup %>% 
  mutate(condition = as.factor(condition),
         participant = as.factor(participant))
```

Let's fit the model: 

```{r}
# fit model
fit.variance = lmer(formula = value ~ 1 + group + condition + (1 | participant),
                data = df.variance)

fit.variance %>% summary()
```

Look at the data and model predictions: 

```{r}
fit.variance %>%
  augment() %>%
  clean_names() %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = value,
                       group = participant)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_point(aes(y = fitted),
             color = "red") +
  geom_line(aes(y = fitted),
             color = "red")
```

And the simulated data: 

```{r}
# simulated data 
fit.variance %>%
  simulate() %>%
  bind_cols(df.mixed) %>%
  ggplot(data = .,
         mapping = aes(x = condition,
                       y = sim_1,
                       group = participant)) +
  geom_line(alpha = 0.5) +
  geom_point(alpha = 0.5)
```

The `lmer()` fails here. It uses one normal distribution to model the variance between participants. It cannot account for the fact that the answers of one group of participants vary more than the answers from another groups of participants. Again, the simulated data doesn't look like the original data, even though we did take the grouping into account. 


## Bootstrapping 

Bootstrapping is a good way to estimate our uncertainty on the parameter estimates in the model. 

### Linear model 

Let's briefly review how to do bootstrapping in a simple linear model. 

```{r}
# fit model 
fit.lm = lm(formula = reaction ~ 1 + days,
            data = df.sleep)

# coefficients
fit.lm %>% coef()

# bootstrapping 
df.boot = df.sleep %>% 
  bootstrap(n = 100,
            id = "id") %>% 
  mutate(fit = map(strap, ~ lm(formula = reaction ~ 1 + days, data = .)),
         tidy = map(fit, tidy)) %>% 
  unnest(tidy) %>% 
  select(id, term, estimate) %>% 
  spread(term, estimate) %>% 
  clean_names() 
```

Let's illustrate the linear model with a confidence interval (making parametric assumptions using the t-distribution). 

```{r}
ggplot(data = df.sleep,
       mapping = aes(x = days, y = reaction)) + 
  geom_smooth(method = "lm") + 
  geom_point(alpha = 0.3)
```

And let's compare this with the different regression lines that we get out of our bootstrapped samples:

```{r}
ggplot(data = df.sleep,
       mapping = aes(x = days, y = reaction)) + 
  geom_abline(data = df.boot,
              aes(intercept = intercept,
                  slope = days,
                  group = id),
              alpha = 0.1) +
  geom_point(alpha = 0.3)
```

#### bootmer() function

For the linear mixed effects model, we can use the `bootmer()` function to do bootstrapping. 

```{r, message=F, warning=F}
set.seed(1)

# fit the model 
fit.lmer = lmer(formula = reaction ~ 1 + days + (1 + days | subject),
                data = df.sleep)

# bootstrap parameter estimates 
boot.lmer = bootMer(fit.lmer,
                    FUN = fixef,
                    nsim = 100)

# compute confidence interval 
boot.ci(boot.lmer, index = 2, type = "perc")

# plot estimates 
boot.lmer$t %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(id = 1:n()) %>% 
  gather("index", "value", - id) %>% 
  ggplot(data = .,
       mapping = aes(x = value)) + 
  geom_density() + 
  facet_grid(cols = vars(index),
             scales = "free") +
  coord_cartesian(expand = F)
```


## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r}
sessionInfo()
```

## References