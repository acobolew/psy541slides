---
title: "Class 21"
author: "Tobias Gerstenberg"
date: ""
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
    pandoc_args: ["--number-offset=20"]
---

```{r setup, include=FALSE}
# these options here change the formatting of how comments are rendered
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  fig.show = "hold")
```

# Bayesian data analysis

In this lecture, we did not perform any Bayesian data analysis. I discussed the costs and benefits of Bayesian analysis and introduced the Bayesian model of multi-modal integration based on the Plinko task. 

## Load packages and set plotting theme  

```{r load-packages, message=FALSE}
library("knitr")      # for knitting RMarkdown 
library("kableExtra") # for making nice tables
library("modelr")     # for permutation test 
library("tidyverse")  # for wrangling, plotting, etc. 
```

```{r set-theme}
theme_set(
  theme_classic() + #set the theme 
    theme(text = element_text(size = 20)) #set the default text size
)
```

## Things that came up 

### Bias in Cosyne 2019 conference admission? 

Code up the data: 

```{r bda1-1}
# data frame 
df.conference = tibble(sex = rep(c("female", "male"), c(264, 677)),
  accepted = rep(c("yes", "no", "yes", "no"), c(83, 264 - 83, 255, 677 - 255))) %>%
  mutate(accepted = factor(accepted, levels = c("no", "yes"), labels = 0:1),
    sex = as.factor(sex))
```

Visualize the results: 

```{r bda1-2}
df.conference %>% 
  ggplot(data = .,
         mapping = aes(x = sex, fill = accepted)) + 
  geom_bar(color = "black") + 
  scale_fill_brewer(palette = "Set1") +
  coord_flip() +
  theme(legend.direction = "horizontal",
        legend.position = "top") + 
  guides(fill = guide_legend(reverse = T))
```

Run a logistic regression with one binary predictor (Binomial test):

```{r bda1-3}
# logistic regression
fit.glm = glm(formula = accepted ~ 1 + sex,
              family = "binomial",
              data = df.conference)

# model summary 
fit.glm %>% 
  summary()
```

The results of the logistic regression are not quite significant (at least when considering a two-tailed test) with $p = .0741$. 

Let's run a permutation test (as suggested by the tweet I showed in class):

```{r bda1-4, cache=TRUE}
# make example reproducible 
set.seed(1)

# difference in proportion 
fun.difference = function(df){
  df %>% 
    as_tibble() %>% 
    count(sex, accepted) %>% 
    group_by(sex) %>% 
    mutate(proportion = n / sum(n)) %>% 
    filter(accepted == 1) %>% 
    select(sex, proportion) %>% 
    spread(sex, proportion) %>% 
    mutate(difference = male - female) %>% 
    pull(difference)  
}

# actual difference 
difference = df.conference %>% 
  fun.difference()

# permutation test 
df.permutation = df.conference %>% 
  permute(n = 1000, sex) %>% 
  mutate(difference = map_dbl(perm, ~ fun.difference(.)))
```

Let's calculate the p-value based on the permutation test: 

```{r bda1-5}
sum(df.permutation$difference > difference) / nrow(df.permutation)
```

And let's visualize the result (showing our observed value and comparing it to the sampling distribution under the null hypothesis):  

```{r bda1-6}
df.permutation %>% 
  ggplot(data = .,
         mapping = aes(x = difference)) +
  stat_density(geom = "line") + 
  geom_vline(xintercept = difference, 
             color = "red",
              size = 1)
```

## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r session}
sessionInfo()
```
